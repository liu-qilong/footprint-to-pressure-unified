{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "from src.dataset.footprint2pressure import Footprint2Pressure_Blend\n",
    "\n",
    "# @DATASET_REGISTRY.register()\n",
    "class Footprint2Pressure_Blend_SensorPatch(Footprint2Pressure_Blend):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        # transformer for resizing images\n",
    "        self.resize = transforms.Resize((self.img_size, self.img_size))\n",
    "\n",
    "        # data indexing\n",
    "        self.index = []\n",
    "\n",
    "        for subject in self.pedar_dynamic.index.get_level_values(1).drop_duplicates():\n",
    "            if os.path.isfile(self.footprint_wrap_folder / f'{subject}-L.jpg'):\n",
    "                for patch_id in range(1, 199):\n",
    "                    self.index.append((subject, patch_id))\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index: int, blend_weight: np.array = None) -> tuple:\n",
    "        subject, sensor_id = self.index[index]\n",
    "        \n",
    "        # blend weights\n",
    "        if blend_weight is None:\n",
    "            blend_weight = np.random.rand(5)\n",
    "            blend_weight = blend_weight / blend_weight.sum()\n",
    "\n",
    "        # weight blends young modulus & pedar arrays\n",
    "        arr_pedar = self.pedar_dynamic.loc[(slice(None), subject), sensor_id].values / self.sense_range\n",
    "        blend_pedar = torch.tensor(\n",
    "            (arr_pedar * blend_weight).sum(axis=0),\n",
    "            dtype=self.dtype,\n",
    "            )\n",
    "        blend_young = torch.tensor(\n",
    "            (np.array(list(self.material_youngs.values())) * blend_weight).sum(),\n",
    "            dtype=self.dtype,\n",
    "            )\n",
    "        \n",
    "        # load footprint image and slice out the sensor-specific patch\n",
    "        if sensor_id < 99:\n",
    "            img = Image.open(self.footprint_wrap_folder / f'{subject}-L.jpg')\n",
    "            img_arr = np.mean(1 - np.array(img).astype(np.float64) / 255, axis=-1)\n",
    "            img_patch = img_arr[self.x_grid['L'][sensor_id - 1], self.y_grid['L'][sensor_id - 1]]\n",
    "\n",
    "        else:\n",
    "            img = Image.open(self.footprint_wrap_folder / f'{subject}-R.jpg')\n",
    "            img_arr = np.mean(1 - np.array(img).astype(np.float64) / 255, axis=-1)\n",
    "            img_patch = img_arr[self.x_grid['L'][sensor_id - 1 - 99], self.y_grid['L'][sensor_id - 1 - 99]]\n",
    "        \n",
    "        img_patch = self.resize(\n",
    "            torch.tensor(img_patch, dtype=self.dtype).unsqueeze(0)\n",
    "            )[0]\n",
    "        \n",
    "        return (img_patch.to(self.device), torch.tensor(sensor_id, dtype=self.dtype).to(self.device), blend_young.to(self.device)), blend_pedar.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9900"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self = Footprint2Pressure_Blend_SensorPatch(\n",
    "    device = device,\n",
    "    footprint_wrap_folder = '../../data/processed/footprint-wrap/',\n",
    "    pedar_dynamic_path = '../../data/processed/pedar_dynamic.pkl',\n",
    "    l_mask_path = '../../data/processed/left_foot_mask.png',\n",
    "    sense_range = 600,\n",
    "    stack_range = 50,\n",
    "    img_size = 10,\n",
    ")\n",
    "len(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.66796875"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9900 / 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 10]),\n",
       " tensor(1., device='mps:0'),\n",
       " tensor(2.2841, device='mps:0'),\n",
       " tensor(0.0575, device='mps:0'))"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(img_patch, sensor_id, blend_young), blend_pedar = self.__getitem__(0)\n",
    "img_patch.shape, sensor_id, blend_young, blend_pedar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.imshow(img_patch.cpu().detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP adapted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from src.model import pos_emb\n",
    "from src.tool.registry import MODEL_REGISTRY\n",
    "\n",
    "from src.model.mlp import MLP_cos_emb\n",
    "\n",
    "# @MODEL_REGISTRY.register()\n",
    "class PatchMLP_cos_emb(MLP_cos_emb):\n",
    "    def forward(self, x):\n",
    "        img_patch, sensor_id, young = x\n",
    "        \n",
    "        # reshape img_stack\n",
    "        infer_shape = img_patch.shape[:-2] + (self.img_size * self.img_size,)  # e.g. (..., 10, 10) -> (..., 100)\n",
    "        img_patch = img_patch.reshape(infer_shape)\n",
    "\n",
    "        # positional embedding\n",
    "        pos_emb = self.position_embedding(sensor_id)  # e.g. (50,) -> (..., 50)\n",
    "        pos_emb = pos_emb.expand(img_patch.shape[:-1] + (-1,))  # e.g. (..., 50) -> (..., 50)\n",
    "\n",
    "        # youngs' modulus embedding\n",
    "        young_emb = self.young_embedding(young)  # (50,) -> (..., 50)\n",
    "\n",
    "        x = torch.cat([img_patch, pos_emb, young_emb], dim=-1)\n",
    "        \n",
    "        return self.mlp(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PatchMLP_cos_emb(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0217, device='mps:0', grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model((img_patch, sensor_id, blend_young))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
